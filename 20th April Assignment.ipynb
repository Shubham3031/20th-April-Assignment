{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07585232-3d0e-48e8-a993-02a1c638b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Ans.\n",
    "\n",
    "KNN (k-Nearest Neighbors) is a popular supervised machine learning algorithm used for classification and\n",
    "regression tasks. It is a non-parametric method that does not make any assumptions about the underlying data\n",
    "distribution.\n",
    "\n",
    "The KNN algorithm works by finding the k closest data points in the training set to the test data point and \n",
    "predicting the class or value of the test data point based on the most common class or average value of the \n",
    "k nearest neighbors. The distance metric used to find the nearest neighbors can be Euclidean distance, Manhattan\n",
    "distance, or any other distance metric that is appropriate for the data.\n",
    "\n",
    "In the case of classification, the KNN algorithm assigns a class to the test data point based on the majority\n",
    "class of the k nearest neighbors. In the case of regression, the KNN algorithm predicts the value of the test\n",
    "data point based on the average of the values of the k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79597956-104f-474a-9bb7-93a3cc185f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Ans.\n",
    "\n",
    "Choosing the right value of k in KNN is crucial to achieve optimal performance of the model.\n",
    "The choice of k can affect the accuracy, bias, and variance of the model.\n",
    "\n",
    "There is no fixed rule for choosing the optimal value of k, and it is usually determined by experimentation\n",
    "and cross-validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b402b-4592-4802-916d-e18e4d4f7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Ans.\n",
    "\n",
    "In KNN classifier, the output is a categorical variable, which means it assigns a class label to a given input\n",
    "data point based on the majority class of its k-nearest neighbors in the feature space. This means that the KNN\n",
    "classifier is useful for classification tasks where the goal is to predict a class label for new data based \n",
    "on the patterns found in the training data.\n",
    "\n",
    "On the other hand, in KNN regressor, the output is a continuous variable, which means it predicts a numerical\n",
    "value based on the average of the k-nearest neighbors. This means that KNN regressor is useful for regression\n",
    "tasks where the goal is to predict a numerical value for new data based on the patterns found in the training \n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d20abe-97a7-4d24-919d-3107e4cbb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Ans.\n",
    "\n",
    "For KNN classification, some of the commonly used evaluation metrics are:\n",
    "\n",
    "Accuracy: It measures the proportion of correctly classified instances over the total number of instances.\n",
    "\n",
    "Precision: It measures the proportion of true positive predictions over the total number of positive\n",
    "predictions.\n",
    "\n",
    "Recall: It measures the proportion of true positive predictions over the total number of actual positive\n",
    "instances.\n",
    "\n",
    "F1 Score: It is the harmonic mean of precision and recall, and provides a balanced evaluation metric that\n",
    "combines both precision and recall.\n",
    "\n",
    "Confusion matrix: It provides a tabular representation of the number of correctly and incorrectly classified \n",
    "instances, allowing for a more detailed analysis of the classifier's performance.\n",
    "\n",
    "For KNN regression, some commonly used evaluation metrics are:\n",
    "\n",
    "Mean Absolute Error (MAE): It measures the average absolute difference between the predicted and actual values.\n",
    "\n",
    "Mean Squared Error (MSE): It measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "Root Mean Squared Error (RMSE): It measures the square root of the average squared difference between the \n",
    "predicted and actual values.\n",
    "\n",
    "R-squared (R2): It measures the proportion of variance in the target variable that is explained by the \n",
    "predictor variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae35a0-9474-409d-921d-6358fac43b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Ans.\n",
    "\n",
    "The curse of dimensionality in KNN refers to the problem of deteriorating performance or accuracy of\n",
    "KNN algorithm as the number of dimensions (features) in the dataset increases. As the number of dimensions\n",
    "increases, the volume of the feature space grows exponentially, which means that the available data becomes\n",
    "sparse, and the distance between the data points becomes more or less similar.\n",
    "\n",
    "This sparsity and similarity make it difficult for KNN to identify the relevant nearest neighbors, and as\n",
    "a result, the KNN algorithm may start to assign more weight to irrelevant neighbors, leading to incorrect\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000660bf-0897-43b3-9ac7-4db89ea20eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Ans.\n",
    "\n",
    "There are several approaches to handle missing values in KNN, including:\n",
    "\n",
    "Removal: One way to handle missing values is to remove the data points that contain missing values.\n",
    "However, this approach can lead to a loss of valuable information and reduce the overall size of the dataset.\n",
    "\n",
    "Imputation: Another way is to replace the missing values with a substitute value. There are various techniques \n",
    "available for imputing missing values, such as mean imputation, median imputation, mode imputation,\n",
    "hot-deck imputation, cold-deck imputation, and k-nearest neighbor imputation.\n",
    "\n",
    "Weighted KNN: An alternative approach is to use a weighted KNN algorithm, which assigns weights to each \n",
    "neighbor based on the distance from the missing value. The weights are then used to calculate the weighted \n",
    "average of the nearest neighbors.\n",
    "\n",
    "Distance-based imputation: This approach involves estimating the missing value based on the similarity between\n",
    "the data point with the missing value and its nearest neighbors. The idea is to estimate the missing value by\n",
    "taking an average of the corresponding attribute values of the K-nearest neighbors, where K is the number of\n",
    "nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794e4ef-4ebd-44ec-8af6-176f8fa97c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Ans.\n",
    "\n",
    "he performance of KNN classifier and regressor depends on the specific problem at hand and the\n",
    "nature of the data. Here are some key differences between KNN classifier and regressor:\n",
    "\n",
    "Output: The KNN classifier predicts a categorical variable, while the KNN regressor predicts a continuous\n",
    "variable.\n",
    "\n",
    "Evaluation metrics: The evaluation metrics used for KNN classifier are different from those used for KNN\n",
    "regressor. For KNN classifier, commonly used metrics include accuracy, precision, recall, F1 score, and \n",
    "confusion matrix. For KNN regressor, commonly used metrics include mean absolute error, mean squared error, \n",
    "root mean squared error, and R-squared.\n",
    "\n",
    "Handling of outliers: KNN regressor is more sensitive to outliers than KNN classifier, as outliers can \n",
    "significantly impact the predicted numerical values. On the other hand, KNN classifier is less sensitive \n",
    "to outliers since it relies on the majority class of the nearest neighbors.\n",
    "\n",
    "Application: KNN classifier is better suited for classification problems where the goal is to predict\n",
    "categorical labels for new data points. Some examples of classification problems include image classification,\n",
    "fraud detection, and sentiment analysis. KNN regressor is better suited for regression problems where the goal\n",
    "is to predict a numerical value for new data points. Some examples of regression problems include stock price \n",
    "prediction, weather forecasting, and housing price prediction.\n",
    "\n",
    "In summary, both KNN classifier and regressor have their strengths and weaknesses, and the choice between \n",
    "them depends on the specific requirements of the problem at hand. It is essential to carefully evaluate the \n",
    "performance of both algorithms and choose the one that provides the best results for the given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca51d3-ff3d-4680-bed9-566d0d3400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Ans.\n",
    "\n",
    "The KNN algorithm has its own strengths and weaknesses for both classification and regression tasks.\n",
    "Here are some of the most important strengths and weaknesses of the KNN algorithm:\n",
    "\n",
    "Strengths of KNN algorithm:\n",
    "\n",
    "Non-parametric: KNN is a non-parametric algorithm, which means it does not make any assumptions about the \n",
    "distribution of the data. This allows it to be used effectively for a wide range of data types, including\n",
    "non-linear and non-normal data.\n",
    "\n",
    "No training phase: KNN does not require a separate training phase, as it simply stores the training data\n",
    "in memory and uses it to make predictions at test time. This makes it easy to implement and computationally \n",
    "efficient.\n",
    "\n",
    "Interpretable: KNN is a simple algorithm that is easy to interpret, making it suitable for applications where\n",
    "transparency is important, such as medical diagnosis or credit scoring.\n",
    "\n",
    "Weaknesses of KNN algorithm:\n",
    "\n",
    "Computationally expensive: KNN can be computationally expensive, especially with large datasets.\n",
    "This is because it requires calculating the distance between the test point and all the training data points,\n",
    "which can be time-consuming.\n",
    "\n",
    "Sensitive to noisy data: KNN can be sensitive to noisy data, as it may consider irrelevant or noisy data points\n",
    "in the vicinity of the test point, leading to incorrect predictions.\n",
    "\n",
    "Curse of dimensionality: As the number of dimensions in the data increases, the distance between the nearest\n",
    "neighbors becomes less meaningful, leading to poorer performance of KNN algorithm. This is known as the curse \n",
    "of dimensionality.\n",
    "\n",
    "Here are some ways to address these weaknesses:\n",
    "\n",
    "Computationally expensive: One way to address this issue is to use a variant of the KNN algorithm, such as\n",
    "ball tree or KD-tree, that can speed up the search for nearest neighbors.\n",
    "\n",
    "Noisy data: One approach to handle noisy data is to remove outliers or use data smoothing techniques, such \n",
    "as moving average or low-pass filtering, to remove noise from the data.\n",
    "\n",
    "Curse of dimensionality: This issue can be addressed by using feature selection or dimensionality reduction \n",
    "techniques to reduce the number of dimensions in the data while retaining most of the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b16cd-b1bf-49e5-8ebe-42e4416aa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Ans.\n",
    "\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm to measure\n",
    "the distance between two data points. Here are the main differences between these two distance metrics:\n",
    "\n",
    "Calculation method: Euclidean distance is calculated as the square root of the sum of squared differences\n",
    "between the corresponding features of two data points. On the other hand, Manhattan distance is calculated\n",
    "as the sum of absolute differences between the corresponding features of two data points.\n",
    "\n",
    "Sensitivity to outliers: Euclidean distance is sensitive to outliers, as it gives more weight to large\n",
    "differences in individual features. On the other hand, Manhattan distance is less sensitive to outliers,\n",
    "as it treats all differences between features equally.\n",
    "\n",
    "Dimensionality: Euclidean distance works well in low-dimensional spaces, but it can suffer from the curse\n",
    "of dimensionality in high-dimensional spaces. On the other hand, Manhattan distance is less affected by\n",
    "the curse of dimensionality, making it a better choice for high-dimensional data.\n",
    "\n",
    "Interpretability: Manhattan distance is more interpretable than Euclidean distance, as it directly measures\n",
    "the distance traveled along each feature axis, while Euclidean distance does not have a direct interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc56c4-a88d-463f-9d42-fe0a6497dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.Ans.\n",
    "\n",
    "Feature scaling is used to normalize the features to a common scale. This can be done using techniques such \n",
    "as Min-Max scaling, where the features are scaled to a range between 0 and 1, or standardization, where the\n",
    "features are transformed to have a mean of 0 and a standard deviation of 1. By scaling the features, all\n",
    "features will have equal importance in the distance calculation, and the KNN algorithm will be better able to \n",
    "capture the true underlying patterns in the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
